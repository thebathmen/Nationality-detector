{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nationality Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Importing Libraries and Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset and preprocess\n",
    "def load_utkface(dataset_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(dataset_path):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            parts = filename.split(\"_\")\n",
    "            if len(parts) == 4:\n",
    "                age, gender, race, _ = parts\n",
    "            elif len(parts) == 3:\n",
    "                age, gender, race = parts\n",
    "            else:\n",
    "                continue\n",
    "            try:\n",
    "                age = int(age)\n",
    "                gender = int(gender)\n",
    "                race = int(race)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if age < 10 or age > 60:\n",
    "                continue\n",
    "            img_path = os.path.join(dataset_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, (48, 48))\n",
    "            data.append(img)\n",
    "            labels.append((age, gender, race))\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "dataset_path = r\"C:\\Users\\Addmin\\Downloads\\Nationality prediction\\UTKFace\"\n",
    "X, y = load_utkface(dataset_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X / 255.0  # Normalize the images\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Separate labels into age, gender, and race\n",
    "y_train_age = y_train[:, 0]\n",
    "y_train_gender = y_train[:, 1]\n",
    "y_train_race = y_train[:, 2]\n",
    "\n",
    "y_test_age = y_test[:, 0]\n",
    "y_test_gender = y_test[:, 1]\n",
    "y_test_race = y_test[:, 2]\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_train_gender = to_categorical(y_train_gender, num_classes=2)\n",
    "y_train_race = to_categorical(y_train_race, num_classes=5)\n",
    "\n",
    "y_test_gender = to_categorical(y_test_gender, num_classes=2)\n",
    "y_test_race = to_categorical(y_test_race, num_classes=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 48, 48, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 46, 46, 32)   896         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 23, 23, 32)   0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 21, 21, 64)   18496       ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 10, 10, 64)  0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 6400)         0           ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          819328      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " age_output (Dense)             (None, 1)            129         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " gender_output (Dense)          (None, 2)            258         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " race_output (Dense)            (None, 5)            645         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 839,752\n",
      "Trainable params: 839,752\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(48, 48, 3))\n",
    "\n",
    "# Convolutional layers\n",
    "x = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Output layers\n",
    "age_output = Dense(1, name='age_output')(x)\n",
    "gender_output = Dense(2, activation='softmax', name='gender_output')(x)\n",
    "race_output = Dense(5, activation='softmax', name='race_output')(x)\n",
    "\n",
    "# Create model\n",
    "model = Model(inputs=input_layer, outputs=[age_output, gender_output, race_output])\n",
    "model.compile(optimizer='adam', loss={'age_output': 'mse', 'gender_output': 'categorical_crossentropy', 'race_output': 'categorical_crossentropy'}, metrics={'age_output': 'mae', 'gender_output': 'accuracy', 'race_output': 'accuracy'})\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Save the model\n",
    "model.save('nationality_prediction_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "457/457 [==============================] - 83s 168ms/step - loss: 187.5372 - age_output_loss: 182.9638 - gender_output_loss: 1.4112 - race_output_loss: 3.1624 - age_output_mae: 10.5740 - gender_output_accuracy: 0.5177 - race_output_accuracy: 0.2865 - val_loss: 139.8442 - val_age_output_loss: 137.7639 - val_gender_output_loss: 0.6617 - val_race_output_loss: 1.4186 - val_age_output_mae: 8.7080 - val_gender_output_accuracy: 0.5614 - val_race_output_accuracy: 0.4121\n",
      "Epoch 2/50\n",
      "457/457 [==============================] - 71s 155ms/step - loss: 140.2910 - age_output_loss: 138.1059 - gender_output_loss: 0.6843 - race_output_loss: 1.5008 - age_output_mae: 9.1794 - gender_output_accuracy: 0.5604 - race_output_accuracy: 0.3751 - val_loss: 118.6668 - val_age_output_loss: 116.5938 - val_gender_output_loss: 0.6573 - val_race_output_loss: 1.4157 - val_age_output_mae: 8.0196 - val_gender_output_accuracy: 0.5564 - val_race_output_accuracy: 0.4112\n",
      "Epoch 3/50\n",
      "457/457 [==============================] - 87s 191ms/step - loss: 131.0426 - age_output_loss: 128.9124 - gender_output_loss: 0.6725 - race_output_loss: 1.4578 - age_output_mae: 8.8688 - gender_output_accuracy: 0.5833 - race_output_accuracy: 0.3918 - val_loss: 98.0458 - val_age_output_loss: 96.0068 - val_gender_output_loss: 0.6429 - val_race_output_loss: 1.3962 - val_age_output_mae: 7.4441 - val_gender_output_accuracy: 0.5734 - val_race_output_accuracy: 0.4112\n",
      "Epoch 4/50\n",
      "457/457 [==============================] - 74s 162ms/step - loss: 125.2873 - age_output_loss: 123.1725 - gender_output_loss: 0.6630 - race_output_loss: 1.4519 - age_output_mae: 8.6446 - gender_output_accuracy: 0.6005 - race_output_accuracy: 0.3927 - val_loss: 127.1430 - val_age_output_loss: 125.0639 - val_gender_output_loss: 0.6514 - val_race_output_loss: 1.4277 - val_age_output_mae: 8.4266 - val_gender_output_accuracy: 0.6049 - val_race_output_accuracy: 0.4926\n",
      "Epoch 5/50\n",
      "457/457 [==============================] - 72s 158ms/step - loss: 118.8198 - age_output_loss: 116.7081 - gender_output_loss: 0.6516 - race_output_loss: 1.4601 - age_output_mae: 8.4442 - gender_output_accuracy: 0.6204 - race_output_accuracy: 0.3913 - val_loss: 84.9172 - val_age_output_loss: 82.8932 - val_gender_output_loss: 0.6281 - val_race_output_loss: 1.3959 - val_age_output_mae: 6.9936 - val_gender_output_accuracy: 0.6619 - val_race_output_accuracy: 0.4112\n",
      "Epoch 6/50\n",
      "457/457 [==============================] - 77s 168ms/step - loss: 110.6320 - age_output_loss: 108.5405 - gender_output_loss: 0.6464 - race_output_loss: 1.4452 - age_output_mae: 8.1351 - gender_output_accuracy: 0.6280 - race_output_accuracy: 0.3939 - val_loss: 77.5627 - val_age_output_loss: 75.5584 - val_gender_output_loss: 0.6192 - val_race_output_loss: 1.3850 - val_age_output_mae: 6.7485 - val_gender_output_accuracy: 0.6781 - val_race_output_accuracy: 0.4556\n",
      "Epoch 7/50\n",
      "457/457 [==============================] - 61s 134ms/step - loss: 104.5527 - age_output_loss: 102.4732 - gender_output_loss: 0.6442 - race_output_loss: 1.4353 - age_output_mae: 7.9166 - gender_output_accuracy: 0.6360 - race_output_accuracy: 0.3971 - val_loss: 76.6639 - val_age_output_loss: 74.6585 - val_gender_output_loss: 0.6072 - val_race_output_loss: 1.3981 - val_age_output_mae: 6.9349 - val_gender_output_accuracy: 0.7033 - val_race_output_accuracy: 0.4118\n",
      "Epoch 8/50\n",
      "457/457 [==============================] - 61s 133ms/step - loss: 102.4854 - age_output_loss: 100.4100 - gender_output_loss: 0.6423 - race_output_loss: 1.4330 - age_output_mae: 7.8165 - gender_output_accuracy: 0.6318 - race_output_accuracy: 0.4030 - val_loss: 73.8930 - val_age_output_loss: 71.9380 - val_gender_output_loss: 0.6025 - val_race_output_loss: 1.3525 - val_age_output_mae: 6.5556 - val_gender_output_accuracy: 0.6852 - val_race_output_accuracy: 0.4310\n",
      "Epoch 9/50\n",
      "457/457 [==============================] - 68s 148ms/step - loss: 99.6021 - age_output_loss: 97.5272 - gender_output_loss: 0.6448 - race_output_loss: 1.4300 - age_output_mae: 7.7303 - gender_output_accuracy: 0.6298 - race_output_accuracy: 0.4057 - val_loss: 72.0673 - val_age_output_loss: 70.0929 - val_gender_output_loss: 0.5993 - val_race_output_loss: 1.3751 - val_age_output_mae: 6.5215 - val_gender_output_accuracy: 0.6970 - val_race_output_accuracy: 0.4115\n",
      "Epoch 10/50\n",
      "457/457 [==============================] - 96s 210ms/step - loss: 95.5705 - age_output_loss: 93.4990 - gender_output_loss: 0.6376 - race_output_loss: 1.4339 - age_output_mae: 7.5393 - gender_output_accuracy: 0.6393 - race_output_accuracy: 0.4018 - val_loss: 85.3612 - val_age_output_loss: 83.3739 - val_gender_output_loss: 0.6079 - val_race_output_loss: 1.3794 - val_age_output_mae: 6.9197 - val_gender_output_accuracy: 0.6759 - val_race_output_accuracy: 0.4307\n",
      "Epoch 11/50\n",
      "457/457 [==============================] - 72s 158ms/step - loss: 93.2481 - age_output_loss: 91.1741 - gender_output_loss: 0.6377 - race_output_loss: 1.4363 - age_output_mae: 7.4371 - gender_output_accuracy: 0.6408 - race_output_accuracy: 0.3953 - val_loss: 71.9322 - val_age_output_loss: 69.9475 - val_gender_output_loss: 0.5965 - val_race_output_loss: 1.3882 - val_age_output_mae: 6.6739 - val_gender_output_accuracy: 0.6973 - val_race_output_accuracy: 0.4112\n",
      "Epoch 12/50\n",
      "457/457 [==============================] - 72s 159ms/step - loss: 93.4060 - age_output_loss: 91.3175 - gender_output_loss: 0.6470 - race_output_loss: 1.4416 - age_output_mae: 7.4657 - gender_output_accuracy: 0.6256 - race_output_accuracy: 0.3900 - val_loss: 68.6621 - val_age_output_loss: 66.6820 - val_gender_output_loss: 0.6044 - val_race_output_loss: 1.3757 - val_age_output_mae: 6.3999 - val_gender_output_accuracy: 0.6940 - val_race_output_accuracy: 0.4110\n",
      "Epoch 13/50\n",
      "457/457 [==============================] - 82s 180ms/step - loss: 89.6200 - age_output_loss: 87.5386 - gender_output_loss: 0.6479 - race_output_loss: 1.4336 - age_output_mae: 7.2686 - gender_output_accuracy: 0.6238 - race_output_accuracy: 0.3981 - val_loss: 69.0062 - val_age_output_loss: 66.9989 - val_gender_output_loss: 0.6263 - val_race_output_loss: 1.3810 - val_age_output_mae: 6.5048 - val_gender_output_accuracy: 0.6630 - val_race_output_accuracy: 0.4510\n",
      "Epoch 14/50\n",
      "457/457 [==============================] - 83s 181ms/step - loss: 88.2381 - age_output_loss: 86.1601 - gender_output_loss: 0.6494 - race_output_loss: 1.4286 - age_output_mae: 7.2345 - gender_output_accuracy: 0.6272 - race_output_accuracy: 0.3984 - val_loss: 78.1040 - val_age_output_loss: 76.0985 - val_gender_output_loss: 0.6187 - val_race_output_loss: 1.3868 - val_age_output_mae: 6.5931 - val_gender_output_accuracy: 0.6586 - val_race_output_accuracy: 0.4112\n",
      "Epoch 15/50\n",
      "457/457 [==============================] - 83s 182ms/step - loss: 86.7151 - age_output_loss: 84.6397 - gender_output_loss: 0.6474 - race_output_loss: 1.4281 - age_output_mae: 7.1842 - gender_output_accuracy: 0.6289 - race_output_accuracy: 0.4016 - val_loss: 65.9678 - val_age_output_loss: 63.9807 - val_gender_output_loss: 0.6307 - val_race_output_loss: 1.3564 - val_age_output_mae: 6.2505 - val_gender_output_accuracy: 0.6282 - val_race_output_accuracy: 0.4140\n",
      "Epoch 16/50\n",
      "457/457 [==============================] - 94s 206ms/step - loss: 83.6712 - age_output_loss: 81.5849 - gender_output_loss: 0.6496 - race_output_loss: 1.4367 - age_output_mae: 7.0199 - gender_output_accuracy: 0.6228 - race_output_accuracy: 0.3962 - val_loss: 67.2216 - val_age_output_loss: 65.2151 - val_gender_output_loss: 0.6195 - val_race_output_loss: 1.3870 - val_age_output_mae: 6.4133 - val_gender_output_accuracy: 0.6778 - val_race_output_accuracy: 0.4112\n",
      "Epoch 17/50\n",
      "457/457 [==============================] - 86s 188ms/step - loss: 85.0072 - age_output_loss: 82.9279 - gender_output_loss: 0.6510 - race_output_loss: 1.4282 - age_output_mae: 7.0478 - gender_output_accuracy: 0.6197 - race_output_accuracy: 0.3978 - val_loss: 74.3343 - val_age_output_loss: 72.2842 - val_gender_output_loss: 0.6564 - val_race_output_loss: 1.3938 - val_age_output_mae: 6.8730 - val_gender_output_accuracy: 0.5904 - val_race_output_accuracy: 0.4112\n",
      "Epoch 18/50\n",
      "457/457 [==============================] - 88s 191ms/step - loss: 81.9119 - age_output_loss: 79.8288 - gender_output_loss: 0.6544 - race_output_loss: 1.4287 - age_output_mae: 6.9570 - gender_output_accuracy: 0.6179 - race_output_accuracy: 0.3974 - val_loss: 66.4906 - val_age_output_loss: 64.5080 - val_gender_output_loss: 0.6335 - val_race_output_loss: 1.3492 - val_age_output_mae: 6.3268 - val_gender_output_accuracy: 0.6463 - val_race_output_accuracy: 0.4189\n",
      "Epoch 19/50\n",
      "457/457 [==============================] - 78s 170ms/step - loss: 78.9862 - age_output_loss: 76.9091 - gender_output_loss: 0.6546 - race_output_loss: 1.4225 - age_output_mae: 6.8087 - gender_output_accuracy: 0.6147 - race_output_accuracy: 0.4027 - val_loss: 71.1591 - val_age_output_loss: 69.1667 - val_gender_output_loss: 0.6391 - val_race_output_loss: 1.3534 - val_age_output_mae: 6.3783 - val_gender_output_accuracy: 0.6260 - val_race_output_accuracy: 0.4112\n",
      "Epoch 20/50\n",
      "457/457 [==============================] - 85s 186ms/step - loss: 79.2422 - age_output_loss: 77.1650 - gender_output_loss: 0.6506 - race_output_loss: 1.4266 - age_output_mae: 6.8240 - gender_output_accuracy: 0.6227 - race_output_accuracy: 0.4017 - val_loss: 65.1292 - val_age_output_loss: 63.1473 - val_gender_output_loss: 0.6228 - val_race_output_loss: 1.3590 - val_age_output_mae: 6.1746 - val_gender_output_accuracy: 0.6688 - val_race_output_accuracy: 0.4112\n",
      "Epoch 21/50\n",
      "457/457 [==============================] - 82s 179ms/step - loss: 75.5569 - age_output_loss: 73.4712 - gender_output_loss: 0.6571 - race_output_loss: 1.4286 - age_output_mae: 6.6777 - gender_output_accuracy: 0.6073 - race_output_accuracy: 0.4026 - val_loss: 71.6785 - val_age_output_loss: 69.6763 - val_gender_output_loss: 0.6277 - val_race_output_loss: 1.3745 - val_age_output_mae: 6.5967 - val_gender_output_accuracy: 0.6559 - val_race_output_accuracy: 0.4112\n",
      "Epoch 22/50\n",
      "457/457 [==============================] - 71s 156ms/step - loss: 74.2852 - age_output_loss: 72.2056 - gender_output_loss: 0.6550 - race_output_loss: 1.4246 - age_output_mae: 6.6008 - gender_output_accuracy: 0.6171 - race_output_accuracy: 0.4040 - val_loss: 64.9129 - val_age_output_loss: 62.9262 - val_gender_output_loss: 0.6330 - val_race_output_loss: 1.3537 - val_age_output_mae: 6.2338 - val_gender_output_accuracy: 0.6532 - val_race_output_accuracy: 0.4112\n",
      "Epoch 23/50\n",
      "457/457 [==============================] - 71s 156ms/step - loss: 74.8020 - age_output_loss: 72.7264 - gender_output_loss: 0.6497 - race_output_loss: 1.4259 - age_output_mae: 6.6305 - gender_output_accuracy: 0.6188 - race_output_accuracy: 0.4038 - val_loss: 66.2452 - val_age_output_loss: 64.2459 - val_gender_output_loss: 0.6076 - val_race_output_loss: 1.3917 - val_age_output_mae: 6.1716 - val_gender_output_accuracy: 0.6816 - val_race_output_accuracy: 0.4137\n",
      "Epoch 24/50\n",
      "457/457 [==============================] - 71s 156ms/step - loss: 72.4556 - age_output_loss: 70.3769 - gender_output_loss: 0.6525 - race_output_loss: 1.4262 - age_output_mae: 6.4893 - gender_output_accuracy: 0.6149 - race_output_accuracy: 0.4027 - val_loss: 65.6628 - val_age_output_loss: 63.6298 - val_gender_output_loss: 0.6449 - val_race_output_loss: 1.3881 - val_age_output_mae: 6.2105 - val_gender_output_accuracy: 0.6227 - val_race_output_accuracy: 0.4236\n",
      "Epoch 25/50\n",
      "457/457 [==============================] - 73s 159ms/step - loss: 70.1827 - age_output_loss: 68.0961 - gender_output_loss: 0.6588 - race_output_loss: 1.4278 - age_output_mae: 6.4085 - gender_output_accuracy: 0.6058 - race_output_accuracy: 0.4002 - val_loss: 66.2689 - val_age_output_loss: 64.2632 - val_gender_output_loss: 0.6331 - val_race_output_loss: 1.3725 - val_age_output_mae: 6.1746 - val_gender_output_accuracy: 0.6329 - val_race_output_accuracy: 0.4112\n",
      "Epoch 26/50\n",
      "457/457 [==============================] - 71s 156ms/step - loss: 70.9462 - age_output_loss: 68.8722 - gender_output_loss: 0.6558 - race_output_loss: 1.4181 - age_output_mae: 6.4476 - gender_output_accuracy: 0.6103 - race_output_accuracy: 0.4055 - val_loss: 66.9187 - val_age_output_loss: 64.9257 - val_gender_output_loss: 0.6180 - val_race_output_loss: 1.3750 - val_age_output_mae: 6.2269 - val_gender_output_accuracy: 0.6655 - val_race_output_accuracy: 0.4112\n",
      "Epoch 27/50\n",
      "457/457 [==============================] - 71s 155ms/step - loss: 68.0101 - age_output_loss: 65.9269 - gender_output_loss: 0.6588 - race_output_loss: 1.4243 - age_output_mae: 6.3083 - gender_output_accuracy: 0.6079 - race_output_accuracy: 0.4078 - val_loss: 73.5401 - val_age_output_loss: 71.5416 - val_gender_output_loss: 0.6272 - val_race_output_loss: 1.3713 - val_age_output_mae: 6.4212 - val_gender_output_accuracy: 0.6655 - val_race_output_accuracy: 0.4112\n",
      "Epoch 28/50\n",
      "457/457 [==============================] - 73s 159ms/step - loss: 68.0322 - age_output_loss: 65.9591 - gender_output_loss: 0.6512 - race_output_loss: 1.4218 - age_output_mae: 6.3214 - gender_output_accuracy: 0.6210 - race_output_accuracy: 0.4077 - val_loss: 65.1531 - val_age_output_loss: 63.1766 - val_gender_output_loss: 0.6239 - val_race_output_loss: 1.3525 - val_age_output_mae: 6.1614 - val_gender_output_accuracy: 0.6389 - val_race_output_accuracy: 0.4893\n",
      "Epoch 29/50\n",
      "457/457 [==============================] - 72s 158ms/step - loss: 65.8566 - age_output_loss: 63.7843 - gender_output_loss: 0.6531 - race_output_loss: 1.4192 - age_output_mae: 6.1841 - gender_output_accuracy: 0.6163 - race_output_accuracy: 0.4077 - val_loss: 66.1995 - val_age_output_loss: 64.2261 - val_gender_output_loss: 0.6210 - val_race_output_loss: 1.3524 - val_age_output_mae: 6.1986 - val_gender_output_accuracy: 0.6537 - val_race_output_accuracy: 0.4860\n",
      "Epoch 30/50\n",
      "457/457 [==============================] - 71s 155ms/step - loss: 64.1610 - age_output_loss: 62.0908 - gender_output_loss: 0.6542 - race_output_loss: 1.4160 - age_output_mae: 6.0866 - gender_output_accuracy: 0.6151 - race_output_accuracy: 0.4126 - val_loss: 65.5445 - val_age_output_loss: 63.5736 - val_gender_output_loss: 0.6241 - val_race_output_loss: 1.3467 - val_age_output_mae: 6.1749 - val_gender_output_accuracy: 0.6603 - val_race_output_accuracy: 0.4252\n",
      "Epoch 31/50\n",
      "457/457 [==============================] - 71s 155ms/step - loss: 63.9117 - age_output_loss: 61.8341 - gender_output_loss: 0.6570 - race_output_loss: 1.4205 - age_output_mae: 6.0813 - gender_output_accuracy: 0.6076 - race_output_accuracy: 0.4071 - val_loss: 67.9665 - val_age_output_loss: 65.9998 - val_gender_output_loss: 0.6097 - val_race_output_loss: 1.3570 - val_age_output_mae: 6.2708 - val_gender_output_accuracy: 0.6830 - val_race_output_accuracy: 0.4195\n",
      "Epoch 32/50\n",
      "457/457 [==============================] - 72s 158ms/step - loss: 63.3630 - age_output_loss: 61.2975 - gender_output_loss: 0.6557 - race_output_loss: 1.4098 - age_output_mae: 6.0956 - gender_output_accuracy: 0.6158 - race_output_accuracy: 0.4142 - val_loss: 65.4620 - val_age_output_loss: 63.4921 - val_gender_output_loss: 0.6257 - val_race_output_loss: 1.3442 - val_age_output_mae: 6.2183 - val_gender_output_accuracy: 0.6715 - val_race_output_accuracy: 0.4134\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, \n",
    "                    {'age_output': y_train_age, 'gender_output': y_train_gender, 'race_output': y_train_race},\n",
    "                    validation_data=(X_test, {'age_output': y_test_age, 'gender_output': y_test_gender, 'race_output': y_test_race}),\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(image):\n",
    "    # Placeholder for emotion prediction function\n",
    "    return \"happy\"\n",
    "\n",
    "def predict_dress_color(image):\n",
    "    # Placeholder for dress color prediction function\n",
    "    return \"red\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_predict(image):\n",
    "    # Preprocess the image\n",
    "    image = cv2.resize(image, (48, 48))\n",
    "    image = np.expand_dims(image, axis=0) / 255.0\n",
    "\n",
    "    # Predict using the model\n",
    "    age_pred, gender_pred, race_pred = model.predict(image)\n",
    "    age = int(age_pred[0])\n",
    "    gender = np.argmax(gender_pred[0])\n",
    "    race = np.argmax(race_pred[0])\n",
    "\n",
    "    emotion = predict_emotion(image)\n",
    "    dress_color = predict_dress_color(image)\n",
    "\n",
    "    if race == 0:  # Assuming race 0 is Indian\n",
    "        result = f\"Nationality: Indian\\nAge: {age}\\nEmotion: {emotion}\\nDress Color: {dress_color}\"\n",
    "    elif race == 1:  # Assuming race 1 is United States\n",
    "        result = f\"Nationality: United States\\nAge: {age}\\nEmotion: {emotion}\"\n",
    "    elif race == 2:  # Assuming race 2 is African\n",
    "        result = f\"Nationality: African\\nEmotion: {emotion}\\nDress Color: {dress_color}\"\n",
    "    else:\n",
    "        result = f\"Nationality: Other\\nEmotion: {emotion}\"\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "def load_image():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        image = cv2.imread(file_path)\n",
    "        result = custom_predict(image)\n",
    "        result_label.config(text=result)\n",
    "\n",
    "app = tk.Tk()\n",
    "app.title(\"Face Attribute Prediction\")\n",
    "\n",
    "upload_button = tk.Button(app, text=\"Upload Image\", command=load_image)\n",
    "upload_button.pack()\n",
    "\n",
    "result_label = tk.Label(app, text=\"\")\n",
    "result_label.pack()\n",
    "\n",
    "app.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, ttk\n",
    "from PIL import Image, ImageTk\n",
    "import cv2\n",
    "\n",
    "# Define a function to load and preprocess the image, then predict the attributes\n",
    "def load_image():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        image = cv2.imread(file_path)\n",
    "        result = custom_predict(image)\n",
    "        \n",
    "        # Display the image in the GUI\n",
    "        image = Image.open(file_path)\n",
    "        image = image.resize((200, 200))\n",
    "        image = ImageTk.PhotoImage(image)\n",
    "        image_label.config(image=image)\n",
    "        image_label.image = image\n",
    "        \n",
    "        result_label.config(text=result)\n",
    "\n",
    "# Define the main application window\n",
    "app = tk.Tk()\n",
    "app.title(\"Nationality Prediction\")\n",
    "\n",
    "# Use a frame for better layout management\n",
    "frame = ttk.Frame(app, padding=\"10\")\n",
    "frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))\n",
    "\n",
    "# Add a title label\n",
    "title_label = ttk.Label(frame, text=\"Nationality Prediction\", font=(\"Helvetica\", 16))\n",
    "title_label.grid(row=0, column=0, columnspan=2, pady=(0, 10))\n",
    "\n",
    "# Add an upload button\n",
    "upload_button = ttk.Button(frame, text=\"Upload Image\", command=load_image)\n",
    "upload_button.grid(row=1, column=0, columnspan=2, pady=10)\n",
    "\n",
    "# Add an image display area\n",
    "image_label = ttk.Label(frame)\n",
    "image_label.grid(row=2, column=0, columnspan=2, pady=(0, 10))\n",
    "\n",
    "# Add a results label\n",
    "result_label = ttk.Label(frame, text=\"\", wraplength=300)\n",
    "result_label.grid(row=3, column=0, columnspan=2)\n",
    "\n",
    "# Configure the grid to expand and fill the window\n",
    "app.columnconfigure(0, weight=1)\n",
    "app.rowconfigure(0, weight=1)\n",
    "frame.columnconfigure(0, weight=1)\n",
    "frame.columnconfigure(1, weight=1)\n",
    "\n",
    "app.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = load_model('nationality_prediction_model.h5')\n",
    "\n",
    "# Define placeholder functions for emotion and dress color prediction\n",
    "def predict_emotion(image):\n",
    "    # Placeholder for emotion prediction function\n",
    "    return \"happy\"\n",
    "\n",
    "def predict_dress_color(image):\n",
    "    # Placeholder for dress color prediction function\n",
    "    return \"red\"\n",
    "\n",
    "def custom_predict(image):\n",
    "    # Preprocess the image\n",
    "    image = cv2.resize(image, (48, 48))\n",
    "    image = np.expand_dims(image, axis=0) / 255.0\n",
    "\n",
    "    # Predict using the model\n",
    "    age_pred, gender_pred, race_pred = model.predict(image)\n",
    "    age = int(age_pred[0])\n",
    "    gender = np.argmax(gender_pred[0])\n",
    "    race = np.argmax(race_pred[0])\n",
    "\n",
    "    emotion = predict_emotion(image)\n",
    "    dress_color = predict_dress_color(image)\n",
    "\n",
    "    if race == 0:  # Assuming race 0 is Indian\n",
    "        result = f\"Nationality: Indian\\nAge: {age}\\nEmotion: {emotion}\\nDress Color: {dress_color}\"\n",
    "    elif race == 1:  # Assuming race 1 is United States\n",
    "        result = f\"Nationality: United States\\nAge: {age}\\nEmotion: {emotion}\"\n",
    "    elif race == 2:  # Assuming race 2 is African\n",
    "        result = f\"Nationality: African\\nEmotion: {emotion}\\nDress Color: {dress_color}\"\n",
    "    else:\n",
    "        result = f\"Nationality: Other\\nEmotion: {emotion}\"\n",
    "\n",
    "    return result\n",
    "\n",
    "# Streamlit app\n",
    "st.title(\"Nationality Prediction\")\n",
    "\n",
    "uploaded_file = st.file_uploader(\"Upload an Image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    # Display the uploaded image\n",
    "    img = Image.open(uploaded_file)\n",
    "    st.image(img, caption=\"Uploaded Image\", use_column_width=True)\n",
    "    \n",
    "    # Convert to OpenCV format\n",
    "    img_cv = np.array(img)\n",
    "    img_cv = cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
